# -*- coding: utf-8 -*-
"""IR-A2-31

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16I8r89ljqA7o5886OmM9TtkpBmv6IDam

#Q1
"""

from google.colab import drive 
drive.mount('/content/gdrive')

PATH = '/content/gdrive/MyDrive/IR/Assignment-1/Humor,Hist,Media,Food/'

import os
import nltk
from nltk.corpus import stopwords
import re
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
from nltk.stem import WordNetLemmatizer
import pandas as pd
import numpy as np
import joblib

FILES = os.listdir(PATH)

def preprocess_file(path_to_file):
  f = open(path_to_file,"r",encoding='latin')
  content = f.read()
  content = re.sub(r'[^\w\s]', ' ', content)#removes punctutations
  content = content.lower()#lowercase
  words = word_tokenize(content)#tokeninzing
  words = [word for word in words if not word in set(stopwords.words('english'))]#removing stop words
  lemmatizer = WordNetLemmatizer()
  word_list=[]
  for token in words:
    lemmatized = lemmatizer.lemmatize(token)
    word_list.append(lemmatized)
  # lemmatized_words = [word for word in word_list if not word in set(stopwords.words('english'))]
  return set(word_list)
  # print(stemmed_words)
  # content=' '.join(words)

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

query = input("Enter your Query: ")

def preprocess_query(query):
  query = re.sub(r'[^\w\s]', ' ', query) #removes punctutations
  query = query.lower()
  words = word_tokenize(query)
  words = [word for word in words if not word in set(stopwords.words('english'))]
  # stemmer = PorterStemmer()
  lemmatizer = WordNetLemmatizer()
  word_list=[]
  for token in words:
    lemmatized = lemmatizer.lemmatize(token)
    word_list.append(lemmatized)
    # query_words = [word for word in word_list if not word in set(stopwords.words('english'))]
  return set(word_list)

proc_query = preprocess_query(query)

def getJaccardCoefficient(proc_query,proc_file):
  return len(proc_query.intersection(proc_file))/len(proc_query.union(proc_file))

# jaccard_coeffs = {}
# for i in range(len(FILES)):
#   proc_file = preprocess_file(PATH+FILES[i])
#   jaccard_coeffs[i] = getJaccardCoefficient(proc_query,proc_file)

# joblib.dump(jaccard_coeffs,"/content/gdrive/MyDrive/IR/Assignment-1/jaccard_coeffs.pkl")
jaccard_coeffs = joblib.load("/content/gdrive/MyDrive/IR/Assignment-1/jaccard_coeffs.pkl")

sorted_JC = list( sorted(jaccard_coeffs.items(), key=lambda x: x[1],reverse=True))
# print(sorted_JC[:5])
top_five=[]
for tups in sorted_JC[:5]:
  top_five.append(tups[0])
# top_five
for f in top_five:
  print(FILES[f])

"""##TF-IDF"""

def preprocess_file_tf(path_to_file):
  f = open(path_to_file,"r",encoding='latin')
  content = f.read()
  content = re.sub(r'[^\w\s]', ' ', content)#removes punctutations
  content = content.lower()#lowercase
  words = word_tokenize(content)#tokeninzing
  words = [word for word in words if not word in set(stopwords.words('english'))]#removing stop words
  lemmatizer = WordNetLemmatizer()
  word_list=[]
  for token in words:
    lemmatized = lemmatizer.lemmatize(token)
    word_list.append(lemmatized)
  # lemmatized_words = [word for word in word_list if not word in set(stopwords.words('english'))]
  return word_list

# tf = {}
# doc_id=0
# for file in FILES:
#   word_list = preprocess_file_tf(PATH+file)
#   tf_temp={}
#   for word in word_list:
#     if word in tf_temp:
#       tf_temp[word] += 1
#     else:
#       tf_temp[word] = 1
#   tf[doc_id] = tf_temp
#   doc_id +=1

# joblib.dump(tf,'/content/gdrive/MyDrive/IR/Assignment-1/'+"tf.pkl")
tf = joblib.load("/content/gdrive/MyDrive/IR/Assignment-1/tf.pkl")

# def add_to_inverted_index(stemmed_word_list,doc_id):
#   for word in stemmed_word_list:
#     if word in inverted_index:
#       inverted_index[word].append(doc_id)
#     else:
#       inverted_index[word] = [doc_id]

# inverted_index = {}
# doc_id=0
# for file_name in FILES:
#   word_list = preprocess_file(PATH+file_name)
#   add_to_inverted_index(word_list,doc_id)
#   doc_id+=1

# import joblib
# joblib.dump(inverted_index,'/content/gdrive/MyDrive/IR/Assignment-1/'+"ii.pkl")
inverted_index = joblib.load("/content/gdrive/MyDrive/IR/Assignment-1/ii.pkl")

df = {}
for k,v in inverted_index.items():
  df[k] = len(v)
# len(df)

idf={}
def getIDF(df):
  for k in df.keys():
    idf[k] = np.log10(len(FILES)/(df[k]+1))
    # print(k)

getIDF(df)
# idf
# df.keys()

vocab = list(inverted_index.keys())
vocab_size = len(vocab)
# vocab_size

def getWordCounts(PATH,doc_name):
  return len(preprocess_file_tf(PATH+doc_name))

getWordCounts(PATH,FILES[2])

sum(list(tf[2].values()))

####### Raw Count ######
# # tf_idf_matrix =[]
# tf_idf = pd.DataFrame(columns = vocab)
# doc_id=0
# for i in range(len(FILES)):
#   # row=[]
#   temp = {}
#   for j in range(vocab_size):
#     try:
#       val = tf[doc_id][vocab[j]] * idf[vocab[j]]
#     except:
#       val = 0
#     # row.append(val)
#     temp[vocab[j]] = val
#     # print(temp)
#   # print(len(temp))
#   tf_idf = tf_idf.append(temp,ignore_index=True)
#   # tf_idf_matrix.append(row)
#   doc_id += 1
#   # print(temp)
# joblib.dump(tf_idf,"/content/gdrive/MyDrive/IR/Assignment-1/raw_count.pkl")
tf_idf = joblib.load("/content/gdrive/MyDrive/IR/Assignment-1/raw_count.pkl")

###### Binary #####
# # tf_idf_matrix =[]
# tf_idf_bin = pd.DataFrame(columns = vocab)
# doc_id=0
# for i in range(len(FILES)):
#   # row=[]
#   temp = {}
#   for j in range(vocab_size):
#     try:
#       val = tf[doc_id][vocab[j]] * idf[vocab[j]]
#       val=1 * idf[vocab[j]]
#     except:
#       val = 0
#     # row.append(val)
#     temp[vocab[j]] = val
#     # print(temp)
#   # print(len(temp))
#   tf_idf_bin = tf_idf_bin.append(temp,ignore_index=True)
#   # tf_idf_matrix.append(row)
#   doc_id += 1
#   # print(temp)
# joblib.dump(tf_idf_bin,"/content/gdrive/MyDrive/IR/Assignment-1/binary.pkl")
tf_idf_bin = joblib.load("/content/gdrive/MyDrive/IR/Assignment-1/binary.pkl")

####### Term Frequency Count ######
# # tf_idf_matrix =[]
# tf_idf_tf = pd.DataFrame(columns = vocab)
# # tf_idf_tf = []
# # doc_id=0
# for doc_id in range(len(FILES)):
#   # row=[]
#   temp = {}
#   for j in range(vocab_size):
#     try:
#     # if vocab[j] in tf[doc_id]:
#       # print(doc_id,j,"inside try")
#       val = (tf[doc_id][vocab[j]] / float(sum(list(tf[2].values())))) * idf[vocab[j]]
#     except:
#     # else:
#       val = 0
#     temp[vocab[j]] = val
#     # row.append(val)
#     # temp.insert(temp,val,1)
#     # temp.append(val)
#   # print(temp)
#   # print(len(temp))
#   # tf_idf_tf = tf_idf_tf.append(temp,ignore_index=True)
#   tf_idf_tf=tf_idf_tf.append(temp,ignore_index=True)
#   # print(doc_id)
#   # tf_idf_matrix.append(row)
#   doc_id += 1
#   # print(temp)
# joblib.dump(tf_idf_tf,"/content/gdrive/MyDrive/IR/Assignment-1/tf_idf_tf.pkl")

# joblib.dump(tf_idf_tf,"/content/gdrive/MyDrive/IR/Assignment-1/tf_idf_tf.pkl")
tf_idf_tf = joblib.load("/content/gdrive/MyDrive/IR/Assignment-1/tf_idf_tf.pkl")

tf_idf_tf.head()

####### Log normalization Count ######
# # tf_idf_matrix =[]
# tf_idf_ln = pd.DataFrame(columns = vocab)
# # doc_id=0
# for doc_id in range(len(FILES)):
#   # row=[]
#   temp = {}
#   for j in range(vocab_size):
#     try:
#     # if vocab[j] in tf[doc_id]:
#       # print(doc_id,j,"inside try")
#       val = np.log(1+tf[doc_id][vocab[j]]) * idf[vocab[j]]
#     except:
#     # else:
#       val = 0
#     # row.append(val)
#     temp[vocab[j]] = val
#   # print(doc_id)
#   # print(len(temp))
#   tf_idf_ln = tf_idf_ln.append(temp,ignore_index=True)
#   # tf_idf_matrix.append(row)
#   doc_id += 1
#   # print(temp)
# joblib.dump(tf_idf_ln,"/content/gdrive/MyDrive/IR/Assignment-1/ln.pkl")
tf_idf_ln = joblib.load("/content/gdrive/MyDrive/IR/Assignment-1/ln.pkl")

tf_idf_ln.head()

####### Double normalization Count ######
# # tf_idf_matrix =[]
# tf_idf_dn = pd.DataFrame(columns = vocab)
# # doc_id=0
# for doc_id in range(len(FILES)):
#   # row=[]
#   temp = {}
#   for j in range(vocab_size):
#     try:
#     # if vocab[j] in tf[doc_id]:
#       # print(doc_id,j,"inside try")
#       val = (0.5+0.5*tf[doc_id][vocab[j]]/max(list(tf[doc_id].values()))) * idf[vocab[j]]
#     except:
#     # else:
#       val = 0
#     # row.append(val)
#     temp[vocab[j]] = val
#   # print(doc_id)
#   # print(len(temp))
#   tf_idf_dn = tf_idf_dn.append(temp,ignore_index=True)
#   # tf_idf_matrix.append(row)
#   doc_id += 1
#   # print(temp)
# joblib.dump(tf_idf_dn,"/content/gdrive/MyDrive/IR/Assignment-1/dn.pkl")
tf_idf_dn = joblib.load("/content/gdrive/MyDrive/IR/Assignment-1/dn.pkl")

tf_idf_dn.head()

query = input("Enter your query: ")

w=preprocess_query(query)
w

query_vec=[]
for word in vocab:
  if word in w:
    query_vec.append(1)
  else:
    query_vec.append(0)
query_vec = np.array(query_vec)

query_vec.shape

#raw count
temp = np.dot(tf_idf,query_vec)
d = {}
for i in range(temp.shape[0]):
  d[FILES[i]] = temp[i]
sorted_rc = list( sorted(d.items(), key=lambda x: x[1],reverse=True))
print(sorted_rc[:5])

#binary
temp = np.dot(tf_idf_bin,query_vec)
d = {}
for i in range(temp.shape[0]):
  d[FILES[i]] = temp[i]
sorted_rc = list( sorted(d.items(), key=lambda x: x[1],reverse=True))
print(sorted_rc[:5])

#Term Frequency
temp = np.dot(tf_idf_tf,query_vec)
d = {}
for i in range(temp.shape[0]):
  d[FILES[i]] = temp[i]
sorted_rc = list( sorted(d.items(), key=lambda x: x[1],reverse=True))
print(sorted_rc[:5])

#Log Normalisation
temp = np.dot(tf_idf_ln,query_vec)
d = {}
for i in range(temp.shape[0]):
  d[FILES[i]] = temp[i]
sorted_rc = list( sorted(d.items(), key=lambda x: x[1],reverse=True))
print(sorted_rc[:5])

#Double Normalisation
temp = np.dot(tf_idf_dn,query_vec)
d = {}
for i in range(temp.shape[0]):
  d[FILES[i]] = temp[i]
sorted_rc = list( sorted(d.items(), key=lambda x: x[1],reverse=True))
print(sorted_rc[:5])

"""# Q2"""

path = "/content/gdrive/MyDrive/IR/Assignment-2/"

import os
import numpy as np
import pandas as pd

data=pd.read_csv(path+'IR-assignment-2-data.txt', sep=' ', header=None,names=["Relevance","Query","Col1","Col2","Col3","Col4","Col5","Col6","Col7","Col8","Col9","Col10","Col11","Col12","Col13","Col14","Col15","Col16","Col17","Col18","Col19","Col20","Col21","Col22","Col23","Col24","Col25","Col26","Col27","Col28","Col29","Col30","Col31","Col32","Col33","Col34","Col35","Col36","Col37","Col38","Col39","Col40","Col41","Col42","Col43","Col44","Col45","Col46","Col47","Col48","Col49","Col50","Col51","Col52","Col53","Col54","Col55","Col56","Col57","Col58","Col59","Col60","Col61","Col62","Col63","Col64","Col65","Col66","Col67","Col68","Col69","Col70","Col71","Col72","Col73","Col74","Col75","Col76","Col77","Col78","Col79","Col80","Col81","Col82","Col83","Col84","Col85","Col86","Col87","Col88","Col89","Col90","Col91","Col92","Col93","Col94","Col95","Col96","Col97","Col98","Col99","Col100","Col101","Col102","Col103","Col104","Col105","Col106","Col107","Col108","Col109","Col110","Col111","Col112","Col113","Col114","Col115","Col116","Col117","Col118","Col119","Col120","Col121","Col122","Col123","Col124","Col125","Col126","Col127","Col128","Col129","Col130","Col131","Col132","Col133","Col134","Col135","Col136","Col137"])

data.shape

data = data.drop(['Col137'],axis=1)

"""##part-1"""

df = data[data['Query']=='qid:4'] #This is the working dataframe

df

df.shape

df.Relevance.unique() #Different order of relevance

"""##part-2"""

result = df.sort_values(by=['Relevance'],ascending=False)

result

result_unique = result.drop_duplicates()

result_unique.shape

result.shape

x = result['Relevance'].value_counts()
print(x)

def factorial(n):
     
    # single line to find factorial
    return 1 if (n==1 or n==0) else n * factorial(n - 1);

total=1
for i in x:
  print(i)
  total = total*(factorial(i))

total

"""##part-3

###part-3-a
"""

df

df_at_50 = df.head(50)

df_at_50

df_result_50 =  df_at_50.sort_values(by=['Relevance'],ascending=False) #sorted data

df_result_50 = df_result_50.reset_index() # resetting index to 0,1,2,3...49

df_result_50

x = df_result_50['Relevance'].value_counts()
print(x)

df_result_50.shape[0]

# from typing import ForwardRef
def calc_ndgc(df1):
  print(df1.shape)
  dgc = 0
  for index, row in df1.iterrows():
    # print(row['level_0'], row['Relevance'])
    if index == 0:
      dgc = dgc + row['Relevance']
      continue
    dgc = dgc + (row['Relevance']/np.log2(index+1))
  return dgc
    
  # for i in range(df_result_50.shape[0]):
  #   print(df1[i]['Relevance'])
  #   print(i+1)

df_result_50

Max_dgc50 = calc_ndgc(df_result_50)

Max_dgc50

df_at_50

df_at_50 = df_at_50.reset_index()

dgc50 = calc_ndgc(df_at_50)

dgc50

ndcg50 = dgc50/Max_dgc50
print(ndcg50)

"""###part-3-b"""

df = df.reset_index()

whole_dataSet_dgc = calc_ndgc(df)

df.shape

df

result = result.reset_index()

result

whole_dataSet_dgc

result.shape

whole_dataSet_dgc_Max = calc_ndgc(result)

whole_dataSet_dgc_Max

ndcg_Whole_dataset = whole_dataSet_dgc/whole_dataSet_dgc_Max

ndcg_Whole_dataset

"""##part-4"""

# example of a precision-recall curve for a predictive model
from sklearn.datasets import make_classification
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_recall_curve
from matplotlib import pyplot

dfpr = df

dfpr

dff = dfpr["Col75"].str.replace("75:","")

dff

dfpr['Col75'] = dff

dfpr[['Col75']] = dfpr[['Col75']].apply(pd.to_numeric)

dfpr_sorted = dfpr.sort_values(by=['Col75'],ascending=False) #sorted data

dfpr_sorted[['Relevance','Col75']]

dfpr_sorted

dfpr_ranked = dfpr_sorted[['index','Relevance','Col75']]

dfpr_ranked

for index, row in dfpr_ranked.iterrows():
    if(row['Relevance']!=0):
      dfpr_ranked.at[index,'Relevance'] = 1
        # row['Relevance']=1

dfpr_ranked

dfpr_ranked.Relevance.unique()

dfpr_ranked

prec_list=[]
rec_list=[]

totalrel = dfpr_ranked.Relevance.value_counts()[1]
print(totalrel)

counter=0
relsofar = 0

for index, row in dfpr_ranked.iterrows():
  # print(index)
  counter+=1
  if(row['Relevance']==1):
    relsofar+=1
  prec_list.append(relsofar/counter)
  rec_list.append(relsofar/totalrel)

prec_list

rec_list

import matplotlib.pyplot as plt

plt.plot(rec_list,prec_list)
plt.xlabel('recall')
plt.ylabel('Precision')
plt.legend()
plt.show()

"""#Q-3"""

path1 = "/content/drive/MyDrive/IR/Assignment-2/Q3/comp.graphics/"
path2 = "/content/drive/MyDrive/IR/Assignment-2/Q3/rec.sport.hockey/" 
path3 = "/content/drive/MyDrive/IR/Assignment-2/Q3/sci.med/"
path4 = "/content/drive/MyDrive/IR/Assignment-2/Q3/sci.space/"
path5 = "/content/drive/MyDrive/IR/Assignment-2/Q3/talk.politics.misc/"

listF1 = os.listdir(path1)
listF2 = os.listdir(path2)
listF3 = os.listdir(path3)
listF4 = os.listdir(path4)
listF5 = os.listdir(path5)

print(len(listF1))
print(len(listF2))
print(len(listF3))
print(len(listF4))
print(len(listF5))

ratio = 0.2

import numpy as np
from sklearn.model_selection import train_test_split

df1 = pd.DataFrame(listF1,columns=['Doc'])
df1['Class'] = 0
# df1
df2 = pd.DataFrame(listF2,columns=['Doc'])
df2['Class'] = 1

df3 = pd.DataFrame(listF3,columns=['Doc'])
df3['Class'] = 2

df4 = pd.DataFrame(listF4,columns=['Doc'])
df4['Class'] = 3

df5 = pd.DataFrame(listF5,columns=['Doc'])
df5['Class'] = 4

df5

X_train1, X_test1, y_train1, y_test1 = train_test_split(df1,df1['Class'], test_size=ratio, random_state=42)
X_train2, X_test2, y_train2, y_test2 = train_test_split(df2,df2['Class'], test_size=ratio, random_state=42)
X_train3, X_test3, y_train3, y_test3 = train_test_split(df3,df3['Class'], test_size=ratio, random_state=42)
X_train4, X_test4, y_train4, y_test4 = train_test_split(df4,df4['Class'], test_size=ratio, random_state=42)
X_train5, X_test5, y_train5, y_test5 = train_test_split(df5,df5['Class'], test_size=ratio, random_state=42)

#perforiming the preprocessing of the files
def preprocess_file(path_to_file):
  f = open(path_to_file,"r",encoding ='latin1')
  content = f.read()
  content = content.lower()#lower case converted 
  content = re.sub(r'[^\w\s]', ' ', content) #punctuation and space
  words = word_tokenize(content) #word tokenization done
  words = [word for word in words if not word in set(stopwords.words('english'))] #removing stopwords
  lemmatizer = WordNetLemmatizer() 
  word_list_2=[]
  for token in words:
    lemmatized = lemmatizer.lemmatize(token)
    word_list_2.append(lemmatized)
  return word_list_2

files1 = os.listdir(path1)
files2 = os.listdir(path2)
files3 = os.listdir(path3)
files4 = os.listdir(path4)
files5 = os.listdir(path5)

word_list_1 = []
for file_name in df1['Doc']:
  print(file_name)
  #preprocessing each file
  word_list_1.extend(preprocess_file(path1+file_name))

word_list_2 = []
for file_name in df2['Doc']:
  print(file_name)
  #preprocessing each file
  word_list_2.extend(preprocess_file(path2+file_name))

word_list_3 = []
for file_name in df3['Doc']:
  print(file_name)
  #preprocessing each file
  word_list_3.extend(preprocess_file(path3+file_name))

word_list_4 = []
for file_name in df4['Doc']:
  print(file_name)
  #preprocessing each file
  word_list_4.extend(preprocess_file(path4+file_name))

word_list_5 = []
for file_name in df5['Doc']:
  print(file_name)
  #preprocessing each file
  word_list_5.extend(preprocess_file(path5+file_name))

# word_list_1 = []
# for file_name in files1:
#   print(file_name)
#   #preprocessing each file
#   word_list_1.extend(preprocess_file(path1+file_name))

# word_list_2 = []
# for file_name in files2:
#   print(file_name)
#   #preprocessing each file
#   word_list_2.extend(preprocess_file(path2+file_name))

# word_list_3 = []
# for file_name in files3:
#   print(file_name)
#   #preprocessing each file
#   word_list_3.extend(preprocess_file(path3+file_name))

# word_list_4 = []
# for file_name in files4:
#   print(file_name)
#   #preprocessing each file
#   word_list_4.extend(preprocess_file(path4+file_name))

# word_list_5 = []
# for file_name in files5:
#   print(file_name)
#   #preprocessing each file
#   word_list_5.extend(preprocess_file(path5+file_name))

# import pickle

# with open('wordlist1', 'wb') as fp:
#     pickle.dump(word_list_1, fp)

import pickle

with open('/content/drive/MyDrive/IR/Assignment-2/Q3/wordlists/wordlist1', 'wb') as fp:
    pickle.dump(word_list_1, fp)

with open('/content/drive/MyDrive/IR/Assignment-2/Q3/wordlists/wordlist2', 'wb') as fp:
    pickle.dump(word_list_2, fp)

with open('/content/drive/MyDrive/IR/Assignment-2/Q3/wordlists/wordlist3', 'wb') as fp:
    pickle.dump(word_list_3, fp)

with open('/content/drive/MyDrive/IR/Assignment-2/Q3/wordlists/wordlist4', 'wb') as fp:
    pickle.dump(word_list_4, fp)

with open('/content/drive/MyDrive/IR/Assignment-2/Q3/wordlists/wordlist5', 'wb') as fp:
    pickle.dump(word_list_5, fp)

# To read it back:
import pickle
with open ('/content/drive/MyDrive/IR/Assignment-2/Q3/wordlists/wordlist1', 'rb') as fp:
    word_list_1 = pickle.load(fp)
with open ('/content/drive/MyDrive/IR/Assignment-2/Q3/wordlists/wordlist2', 'rb') as fp:
    word_list_2 = pickle.load(fp)
with open ('/content/drive/MyDrive/IR/Assignment-2/Q3/wordlists/wordlist3', 'rb') as fp:
    word_list_3 = pickle.load(fp)
with open ('/content/drive/MyDrive/IR/Assignment-2/Q3/wordlists/wordlist4', 'rb') as fp:
    word_list_4 = pickle.load(fp)
with open ('/content/drive/MyDrive/IR/Assignment-2/Q3/wordlists/wordlist5', 'rb') as fp:
    word_list_5 = pickle.load(fp)

len(word_list_2)

unique_word_set1 = set(word_list_1)
unique_word_set2 = set(word_list_2)
unique_word_set3 = set(word_list_3)
unique_word_set4 = set(word_list_4)
unique_word_set5 = set(word_list_5)

unique_word_set1

from collections import Counter
dic1 = Counter(word_list_1)
dic2 = Counter(word_list_2)
dic3 = Counter(word_list_3)
dic4 = Counter(word_list_4)
dic5 = Counter(word_list_5)

unique_word_set_all_merged = unique_word_set1.union(unique_word_set2)
unique_word_set_all_merged = unique_word_set_all_merged.union(unique_word_set3)
unique_word_set_all_merged = unique_word_set_all_merged.union(unique_word_set4)
unique_word_set_all_merged = unique_word_set_all_merged.union(unique_word_set5)

len(unique_word_set2)

len(unique_word_set_all_merged)

# Class Frequency 
# By Definition the count of classes the word is present in.
cf = {}
for word in unique_word_set_all_merged:
  count = 0
  if word in dic1.keys():
    count+=1
  if word in dic2.keys():
    count+=1
  if word in dic3.keys():
    count+=1
  if word in dic4.keys():
    count+=1
  if word in dic5.keys():
    count+=1
  cf[word] = count

cf

# Inverse Class Frequency
icf = {}
for word in unique_word_set_all_merged:
  icf[word] = np.log10(5/cf[word])

len(unique_word_set_all_merged)

icf

tcidfmatrix = pd.DataFrame(columns=['Doc','Class'])
tcidfmatrix.shape

print(X_train1.shape)
print(X_train2.shape)
print(X_train3.shape)
print(X_train4.shape)
print(.shape)

numofrow = X_train1.shape[0] * 5
print(numofrow)
numofcol = len(unique_word_set_all_merged)+2
print(numofcol)

arr = np.zeros((numofrow,numofcol))

arr

arr.shape[0]

i=0
for ind,row in X_train1.iterrows():
  arr[i][0] = row['Doc']
  arr[i][1] = row['Class']
  i+=1
for ind,row in X_train2.iterrows():
  arr[i][0] = row['Doc']
  arr[i][1] = row['Class']
  i+=1
for ind,row in X_train3.iterrows():
  arr[i][0] = row['Doc']
  arr[i][1] = row['Class']
  i+=1
for ind,row in X_train4.iterrows():
  arr[i][0] = row['Doc']
  arr[i][1] = row['Class']
  i+=1
for ind,row in X_train5.iterrows():
  arr[i][0] = row['Doc']
  arr[i][1] = row['Class']
  i+=1

len(unique_word_set_all_merged)

wordNameIndexMap = {}
index = 2
for word in unique_word_set_all_merged:
  wordNameIndexMap[index]=word
  index+=1

print(numofrow)
print(numofcol)

# wordNameIndexMap
for i in range(numofrow):
  print(i)
  for j in range(2,numofcol):
    word = wordNameIndexMap[j]#picked up word from dictionary 
    classId = arr[i][1]
    #find tc and icf of it depending upon its class
    val = 0.0
    if classId == 0:
      try:
        val = dic1[word] * icf[word]
      except:
        val=0.0
    elif classId == 1:
      try:
        val = dic2[word] * icf[word]
      except:
        val=0.0
    elif classId == 2:
      try:
        val = dic3[word] * icf[word]
      except:
        val=0.0
    elif classId == 3:
      try:
        val = dic4[word] * icf[word]
      except:
        val=0.0
    elif classId == 4:
      try:
        val = dic5[word] * icf[word]
      except:
        val=0.0
    arr[i][j]=val

# for index,row in tcidfmatrix.iterrows():
#   docid = row['Doc']
#   classId = row['Class']
#   temp = {}
#   for word in unique_word_set_all_merged:
#     val=0.0
#     if classId == 0:
#       try:
#         val = df1[word] * icf[word]
#       except:
#         val=0.0
#     elif classId == 1:
#       try:
#         val = df2[word] * icf[word]
#       except:
#         val=0.0
#     elif classId == 2:
#       try:
#         val = df3[word] * icf[word]
#       except:
#         val=0.0
#     elif classId == 3:
#       try:
#         val = df4[word] * icf[word]
#       except:
#         val=0.0
#     elif classId == 4:
#       try:
#         val = df5[word] * icf[word]
#       except:
#         val=0.0
#     row[word]=val
  

# # for docid in tcidfmatrix['Doc']:
# #   for word in unique_word_set_all_merged:
# #     temp = {}
# #     try:
# #       if tcidfmatrix['Class']==0:

dfarr = pd.DataFrame(arr)

dfarr

dfarr.shape

colmList = ['Doc','Class']

colmList.extend(unique_word_set_all_merged)

len(colmList)

dfarr.columns = colmList



dfarr

dfarr.to_csv(r'/content/drive/MyDrive/IR/Assignment-2/Q3/InitialTCICF.csv', index = False)

# dfarr = pd.read_csv ('/content/drive/MyDrive/IR/Assignment-2/Q3/InitialTCICF.csv')
# print(dfarr)

dfarr

dfcls0 = dfarr[dfarr['Class']==0]
print(dfcls0)
dfcls0 = dfcls0.drop(columns = ['Doc','Class'],axis=1)
print(dfcls0)
sum_row0 = dfcls0.sum(axis = 0, skipna = True)
cls0sum = pd.DataFrame(sum_row0)
print(cls0sum)
cls0sum= cls0sum.sort_values(0,ascending=False)
print(cls0sum)



# Class1
dfcls1 = dfarr[dfarr['Class']==1]
print(dfcls1)
dfcls1 = dfcls1.drop(columns = ['Doc','Class'],axis=1)
print(dfcls1)
sum_row1 = dfcls1.sum(axis = 0, skipna = True)
cls1sum = pd.DataFrame(sum_row1)
print(cls1sum)
cls1sum= cls1sum.sort_values(0,ascending=False)
print(cls1sum)

#Class 2
dfcls2 = dfarr[dfarr['Class']==2]
print(dfcls2)
dfcls2 = dfcls2.drop(columns = ['Doc','Class'],axis=1)
print(dfcls2)
sum_row2 = dfcls2.sum(axis = 0, skipna = True)
cls2sum = pd.DataFrame(sum_row2)
print(cls2sum)
cls2sum= cls2sum.sort_values(0,ascending=False)
print(cls2sum)


# Class3
dfcls3 = dfarr[dfarr['Class']==3]
print(dfcls3)
dfcls3 = dfcls3.drop(columns = ['Doc','Class'],axis=1)
print(dfcls3)
sum_row3 = dfcls3.sum(axis = 0, skipna = True)
cls3sum = pd.DataFrame(sum_row3)
print(cls3sum)
cls3sum= cls3sum.sort_values(0,ascending=False)
print(cls3sum)

# Class4
dfcls4 = dfarr[dfarr['Class']==4]
print(dfcls4)
dfcls4 = dfcls4.drop(columns = ['Doc','Class'],axis=1)
print(dfcls4)
sum_row4 = dfcls4.sum(axis = 0, skipna = True)
cls4sum = pd.DataFrame(sum_row4)
print(cls4sum)
cls4sum= cls4sum.sort_values(0,ascending=False)
print(cls4sum)

# k = input('enter k :')
k=5
topkfeatures = ['Doc','Class']
t = 1
for ind,row in cls0sum.iterrows():
  if t > k :
    break
  print(ind)
  topkfeatures.append(ind)
  t+=1

t = 1
for ind,row in cls1sum.iterrows():
  if t > k :
    break
  print(ind)
  topkfeatures.append(ind)
  t+=1

t = 1
for ind,row in cls2sum.iterrows():
  if t > k :
    break
  print(ind)
  topkfeatures.append(ind)
  t+=1

t = 1
for ind,row in cls3sum.iterrows():
  if t > k :
    break
  print(ind)
  topkfeatures.append(ind)
  t+=1

t = 1
for ind,row in cls4sum.iterrows():
  if t > k :
    break
  print(ind)
  topkfeatures.append(ind)
  t+=1

topkfeatures



topkfeaturesfinal = []
for i in topkfeatures:
  if i in topkfeaturesfinal:
    continue
  else:
    topkfeaturesfinal.append(i)

# topkfeatures = list(set(topkfeatures))
topkfeaturesfinal

topkfeaturesfinal

dffinal = dfarr[(topkfeaturesfinal)]

dffinal

x_trainfinal = dffinal.drop(['Class','Doc'],axis = 1)

x_trainfinal



y_trainfinal = dffinal['Class']

y_trainfinal



X_test1

topkfeaturesfinal

numofrow1 = X_test1.shape[0] * 5
print(numofrow1)
numofcol1 = len(topkfeaturesfinal) #already including doc and class
print(numofcol1)

testarr = np.zeros((numofrow1,numofcol1))

testarr

testarr.shape

i=0
for ind,row in X_test1.iterrows():
  testarr[i][0] = row['Doc']
  testarr[i][1] = row['Class']
  i+=1
for ind,row in X_test2.iterrows():
  testarr[i][0] = row['Doc']
  testarr[i][1] = row['Class']
  i+=1
for ind,row in X_test3.iterrows():
  testarr[i][0] = row['Doc']
  testarr[i][1] = row['Class']
  i+=1
for ind,row in X_test4.iterrows():
  testarr[i][0] = row['Doc']
  testarr[i][1] = row['Class']
  i+=1
for ind,row in X_test5.iterrows():
  testarr[i][0] = row['Doc']
  testarr[i][1] = row['Class']
  i+=1

testarr



topkfeaturesfinal

wordNameIndexMap1 = {}
index = 0
for word in topkfeaturesfinal:#ignore doc and class
  wordNameIndexMap1[index]=word
  index+=1

wordNameIndexMap1

numofcol1

# wordNameIndexMap
for i in range(numofrow1):
  print(i)
  for j in range(2,numofcol1):
    word = wordNameIndexMap1[j]#picked up word from dictionary 
    classId = testarr[i][1]
    #find tc and icf of it depending upon its class
    val = 0.0
    if classId == 0:
      try:
        val = dic1[word] * icf[word]
      except:
        val=0.0
    elif classId == 1:
      try:
        val = dic2[word] * icf[word]
      except:
        val=0.0
    elif classId == 2:
      try:
        val = dic3[word] * icf[word]
      except:
        val=0.0
    elif classId == 3:
      try:
        val = dic4[word] * icf[word]
      except:
        val=0.0
    elif classId == 4:
      try:
        val = dic5[word] * icf[word]
      except:
        val=0.0
    testarr[i][j]=val

dfarrtest = pd.DataFrame(testarr)

dfarrtest

dfarrtest.shape

topkfeaturesfinal

dfarrtest.columns = topkfeaturesfinal

dfarrtest

y_testfinal = dfarrtest['Class']

x_testfinal = dfarrtest.drop(['Class','Doc'],axis = 1)

x_testfinal

y_testfinal

from sklearn.naive_bayes import MultinomialNB

clf = MultinomialNB()
clf.fit(x_trainfinal, y_trainfinal)
MultinomialNB()

class MultiNB(self):
  def fit(self,X_train,y_train):
    row,col = X_train.shape
    self.n = np.unique(y_train)
    self.pri = np.zeros(self.n)
    self.likeli = np.zeros((self.n, col))
    for ind in self.n:
        X_train_label = X_train[ind == y_train]
        self.pri[ind] = X_train_label.shape[0] / row 
        self.likeli[ind, :] = (X_train_label.sum(axis=0)) / (np.sum(X_train_label.sum(axis=0) + 1))

  def predict_one(self, x_test):
    poste = []
    for ind in self.n:
        pri_label = np.log(self.pri[ind])
        likeli_label = np.log(self.likeli[ind,:]) * x_test
        poste_label = np.sum(likeli_label) + pri_label
        poste.append(poste_label)
    return self.n[np.argmax(poste)]

    def predict(self,X_test):
      preds = []
      for row in X_test:
        preds.append(self.predict_one(row))
      return preds

mnb = MultiNB()
clf=mnb.fit(x_trainfinal, y_trainfinal)

y_testpred = clf.predict(x_testfinal)

from sklearn.metrics import accuracy_score

accuracy_score(y_testpred,y_testfinal)

from sklearn.metrics import confusion_matrix
confusion_matrix(y_testpred, y_testfinal)